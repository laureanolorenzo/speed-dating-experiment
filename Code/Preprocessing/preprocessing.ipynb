{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This notebook contains all the necessary work to prepare the dataset for later analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from processing_tools import * #Located within the same directory\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../Datasets/speed_dating.csv'\n",
    "df = pd.read_csv(path,low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.replace('?',np.nan,inplace=True) #Nulls are coded as \"?\", so I just turn them into np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8378 entries, 0 to 8377\n",
      "Data columns (total 123 columns):\n",
      " #    Column                           Dtype \n",
      "---   ------                           ----- \n",
      " 0    has_null                         int64 \n",
      " 1    wave                             int64 \n",
      " 2    gender                           object\n",
      " 3    age                              object\n",
      " 4    age_o                            object\n",
      " 5    d_age                            int64 \n",
      " 6    d_d_age                          object\n",
      " 7    race                             object\n",
      " 8    race_o                           object\n",
      " 9    samerace                         int64 \n",
      " 10   importance_same_race             object\n",
      " 11   importance_same_religion         object\n",
      " 12   d_importance_same_race           object\n",
      " 13   d_importance_same_religion       object\n",
      " 14   field                            object\n",
      " 15   pref_o_attractive                object\n",
      " 16   pref_o_sincere                   object\n",
      " 17   pref_o_intelligence              object\n",
      " 18   pref_o_funny                     object\n",
      " 19   pref_o_ambitious                 object\n",
      " 20   pref_o_shared_interests          object\n",
      " 21   d_pref_o_attractive              object\n",
      " 22   d_pref_o_sincere                 object\n",
      " 23   d_pref_o_intelligence            object\n",
      " 24   d_pref_o_funny                   object\n",
      " 25   d_pref_o_ambitious               object\n",
      " 26   d_pref_o_shared_interests        object\n",
      " 27   attractive_o                     object\n",
      " 28   sinsere_o                        object\n",
      " 29   intelligence_o                   object\n",
      " 30   funny_o                          object\n",
      " 31   ambitous_o                       object\n",
      " 32   shared_interests_o               object\n",
      " 33   d_attractive_o                   object\n",
      " 34   d_sinsere_o                      object\n",
      " 35   d_intelligence_o                 object\n",
      " 36   d_funny_o                        object\n",
      " 37   d_ambitous_o                     object\n",
      " 38   d_shared_interests_o             object\n",
      " 39   attractive_important             object\n",
      " 40   sincere_important                object\n",
      " 41   intellicence_important           object\n",
      " 42   funny_important                  object\n",
      " 43   ambtition_important              object\n",
      " 44   shared_interests_important       object\n",
      " 45   d_attractive_important           object\n",
      " 46   d_sincere_important              object\n",
      " 47   d_intellicence_important         object\n",
      " 48   d_funny_important                object\n",
      " 49   d_ambtition_important            object\n",
      " 50   d_shared_interests_important     object\n",
      " 51   attractive                       object\n",
      " 52   sincere                          object\n",
      " 53   intelligence                     object\n",
      " 54   funny                            object\n",
      " 55   ambition                         object\n",
      " 56   d_attractive                     object\n",
      " 57   d_sincere                        object\n",
      " 58   d_intelligence                   object\n",
      " 59   d_funny                          object\n",
      " 60   d_ambition                       object\n",
      " 61   attractive_partner               object\n",
      " 62   sincere_partner                  object\n",
      " 63   intelligence_partner             object\n",
      " 64   funny_partner                    object\n",
      " 65   ambition_partner                 object\n",
      " 66   shared_interests_partner         object\n",
      " 67   d_attractive_partner             object\n",
      " 68   d_sincere_partner                object\n",
      " 69   d_intelligence_partner           object\n",
      " 70   d_funny_partner                  object\n",
      " 71   d_ambition_partner               object\n",
      " 72   d_shared_interests_partner       object\n",
      " 73   sports                           object\n",
      " 74   tvsports                         object\n",
      " 75   exercise                         object\n",
      " 76   dining                           object\n",
      " 77   museums                          object\n",
      " 78   art                              object\n",
      " 79   hiking                           object\n",
      " 80   gaming                           object\n",
      " 81   clubbing                         object\n",
      " 82   reading                          object\n",
      " 83   tv                               object\n",
      " 84   theater                          object\n",
      " 85   movies                           object\n",
      " 86   concerts                         object\n",
      " 87   music                            object\n",
      " 88   shopping                         object\n",
      " 89   yoga                             object\n",
      " 90   d_sports                         object\n",
      " 91   d_tvsports                       object\n",
      " 92   d_exercise                       object\n",
      " 93   d_dining                         object\n",
      " 94   d_museums                        object\n",
      " 95   d_art                            object\n",
      " 96   d_hiking                         object\n",
      " 97   d_gaming                         object\n",
      " 98   d_clubbing                       object\n",
      " 99   d_reading                        object\n",
      " 100  d_tv                             object\n",
      " 101  d_theater                        object\n",
      " 102  d_movies                         object\n",
      " 103  d_concerts                       object\n",
      " 104  d_music                          object\n",
      " 105  d_shopping                       object\n",
      " 106  d_yoga                           object\n",
      " 107  interests_correlate              object\n",
      " 108  d_interests_correlate            object\n",
      " 109  expected_happy_with_sd_people    object\n",
      " 110  expected_num_interested_in_me    object\n",
      " 111  expected_num_matches             object\n",
      " 112  d_expected_happy_with_sd_people  object\n",
      " 113  d_expected_num_interested_in_me  object\n",
      " 114  d_expected_num_matches           object\n",
      " 115  like                             object\n",
      " 116  guess_prob_liked                 object\n",
      " 117  d_like                           object\n",
      " 118  d_guess_prob_liked               object\n",
      " 119  met                              object\n",
      " 120  decision                         int64 \n",
      " 121  decision_o                       int64 \n",
      " 122  match                            int64 \n",
      "dtypes: int64(7), object(116)\n",
      "memory usage: 7.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info(verbose=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most columns are of type \"object\", but in reality, they should be numeric. This is probably caused by the \n",
    "use of \"?\" as the null representation.To fix it, I created a function called \"convert_str\" and applied \n",
    "it to the whole dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.applymap(convert_str)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some column names have typos, so I went to the dataset description site and grabbed all the columns from there to check them against mine. The dataset description can be found here:\n",
    "[Speed Dating Dataset](https://www.openml.org/search?type=data&sort=runs&status=active&id=40536)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gender', 'age', 'age_o', 'd_age', 'race']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = '''* gender: Gender of self  \n",
    " * age: Age of self  \n",
    " * age_o: Age of partner  \n",
    " * d_age: Difference in age  \n",
    " * race: Race of self  \n",
    " * race_o: Race of partner  \n",
    " * samerace: Whether the two persons have the same race or not.  \n",
    " * importance_same_race: How important is it that partner is of same race?  \n",
    " * importance_same_religion: How important is it that partner has same religion?  \n",
    " * field: Field of study  \n",
    " * pref_o_attractive: How important does partner rate attractiveness  \n",
    " * pref_o_sinsere: How important does partner rate sincerity  \n",
    " * pref_o_intelligence: How important does partner rate intelligence  \n",
    " * pref_o_funny: How important does partner rate being funny  \n",
    " * pref_o_ambitious: How important does partner rate ambition  \n",
    " * pref_o_shared_interests: How important does partner rate having shared interests  \n",
    " * attractive_o: Rating by partner (about me) at night of event on attractiveness  \n",
    " * sincere_o: Rating by partner (about me) at night of event on sincerity  \n",
    " * intelligence_o: Rating by partner (about me) at night of event on intelligence  \n",
    " * funny_o: Rating by partner (about me) at night of event on being funny  \n",
    " * ambitous_o: Rating by partner (about me) at night of event on being ambitious  \n",
    " * shared_interests_o: Rating by partner (about me) at night of event on shared interest  \n",
    " * attractive_important: What do you look for in a partner - attractiveness  \n",
    " * sincere_important: What do you look for in a partner - sincerity  \n",
    " * intellicence_important: What do you look for in a partner - intelligence  \n",
    " * funny_important: What do you look for in a partner - being funny  \n",
    " * ambtition_important: What do you look for in a partner - ambition  \n",
    " * shared_interests_important: What do you look for in a partner - shared interests  \n",
    " * attractive: Rate yourself - attractiveness  \n",
    " * sincere: Rate yourself - sincerity   \n",
    " * intelligence: Rate yourself - intelligence   \n",
    " * funny: Rate yourself - being funny   \n",
    " * ambition: Rate yourself - ambition  \n",
    " * attractive_partner: Rate your partner - attractiveness  \n",
    " * sincere_partner: Rate your partner - sincerity   \n",
    " * intelligence_partner: Rate your partner - intelligence   \n",
    " * funny_partner: Rate your partner - being funny   \n",
    " * ambition_partner: Rate your partner - ambition   \n",
    " * shared_interests_partner: Rate your partner - shared interests  \n",
    " * sports: Your own interests [1-10]  \n",
    " * tvsports  \n",
    " * exercise  \n",
    " * dining  \n",
    " * museums  \n",
    " * art  \n",
    " * hiking  \n",
    " * gaming  \n",
    " * clubbing  \n",
    " * reading  \n",
    " * tv  \n",
    " * theater  \n",
    " * movies  \n",
    " * concerts  \n",
    " * music  \n",
    " * shopping  \n",
    " * yoga  \n",
    " * interests_correlate: Correlation between participant’s and partner’s ratings of interests.  \n",
    " * expected_happy_with_sd_people: How happy do you expect to be with the people you meet during the speed-dating event?  \n",
    " * expected_num_interested_in_me: Out of the 20 people you will meet, how many do you expect will be interested in dating you?  \n",
    " * expected_num_matches: How many matches do you expect to get?  \n",
    " * like: Did you like your partner?  \n",
    " * guess_prob_liked: How likely do you think it is that your partner likes you?   \n",
    " * met: Have you met your partner before?  \n",
    " * decision: Decision at night of event.\n",
    " * decision_o: Decision of partner at night of event.  \n",
    " * match: Match (yes/no)'''\n",
    "cols = cols.split('*')[1:] #Every column is pointed by an \"*\"\n",
    "cols = [i.split(':')[0].strip() for i in cols] #Col names are located to the left of the colons\n",
    "cols[:5] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_w_errors = [i for i in df.columns if i not in cols and not i.startswith('d_')] \n",
    "#The \"d_\" columns were added by the researchers and they don't have a description in the site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['has_null', 'wave', 'pref_o_sincere', 'sinsere_o']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_w_errors #has_null and wave don't belong to the experiment data either, as they were added to describe the dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only 'sinsere_o' needs to be fixed, since 'pref_o_sincere' is spelled wrong in the site. Also, \"ambition_important\" seems to have\n",
    "a typo but it's incorrectly spelled in the site as well. I had to manually detect it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'sinsere_o':'sincere_o','ambtition_important':'ambition_important'},inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We end up with a cleaner dataset, but there is still some work to be done before it's ready for analysis.\n",
    "We'll start by turning categorical columns into \"one-hot encoding\" format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8378 entries, 0 to 8377\n",
      "Data columns (total 123 columns):\n",
      " #    Column                           Dtype  \n",
      "---   ------                           -----  \n",
      " 0    has_null                         int8   \n",
      " 1    wave                             int8   \n",
      " 2    gender                           object \n",
      " 3    age                              float64\n",
      " 4    age_o                            float64\n",
      " 5    d_age                            int8   \n",
      " 6    d_d_age                          object \n",
      " 7    race                             object \n",
      " 8    race_o                           object \n",
      " 9    samerace                         int8   \n",
      " 10   importance_same_race             float64\n",
      " 11   importance_same_religion         float64\n",
      " 12   d_importance_same_race           object \n",
      " 13   d_importance_same_religion       object \n",
      " 14   field                            object \n",
      " 15   pref_o_attractive                float64\n",
      " 16   pref_o_sincere                   float64\n",
      " 17   pref_o_intelligence              float64\n",
      " 18   pref_o_funny                     float64\n",
      " 19   pref_o_ambitious                 float64\n",
      " 20   pref_o_shared_interests          float64\n",
      " 21   d_pref_o_attractive              object \n",
      " 22   d_pref_o_sincere                 object \n",
      " 23   d_pref_o_intelligence            object \n",
      " 24   d_pref_o_funny                   object \n",
      " 25   d_pref_o_ambitious               object \n",
      " 26   d_pref_o_shared_interests        object \n",
      " 27   attractive_o                     float64\n",
      " 28   sincere_o                        float64\n",
      " 29   intelligence_o                   float64\n",
      " 30   funny_o                          float64\n",
      " 31   ambitous_o                       float64\n",
      " 32   shared_interests_o               float64\n",
      " 33   d_attractive_o                   object \n",
      " 34   d_sinsere_o                      object \n",
      " 35   d_intelligence_o                 object \n",
      " 36   d_funny_o                        object \n",
      " 37   d_ambitous_o                     object \n",
      " 38   d_shared_interests_o             object \n",
      " 39   attractive_important             float64\n",
      " 40   sincere_important                float64\n",
      " 41   intellicence_important           float64\n",
      " 42   funny_important                  float64\n",
      " 43   ambition_important               float64\n",
      " 44   shared_interests_important       float64\n",
      " 45   d_attractive_important           object \n",
      " 46   d_sincere_important              object \n",
      " 47   d_intellicence_important         object \n",
      " 48   d_funny_important                object \n",
      " 49   d_ambtition_important            object \n",
      " 50   d_shared_interests_important     object \n",
      " 51   attractive                       float64\n",
      " 52   sincere                          float64\n",
      " 53   intelligence                     float64\n",
      " 54   funny                            float64\n",
      " 55   ambition                         float64\n",
      " 56   d_attractive                     object \n",
      " 57   d_sincere                        object \n",
      " 58   d_intelligence                   object \n",
      " 59   d_funny                          object \n",
      " 60   d_ambition                       object \n",
      " 61   attractive_partner               float64\n",
      " 62   sincere_partner                  float64\n",
      " 63   intelligence_partner             float64\n",
      " 64   funny_partner                    float64\n",
      " 65   ambition_partner                 float64\n",
      " 66   shared_interests_partner         float64\n",
      " 67   d_attractive_partner             object \n",
      " 68   d_sincere_partner                object \n",
      " 69   d_intelligence_partner           object \n",
      " 70   d_funny_partner                  object \n",
      " 71   d_ambition_partner               object \n",
      " 72   d_shared_interests_partner       object \n",
      " 73   sports                           float64\n",
      " 74   tvsports                         float64\n",
      " 75   exercise                         float64\n",
      " 76   dining                           float64\n",
      " 77   museums                          float64\n",
      " 78   art                              float64\n",
      " 79   hiking                           float64\n",
      " 80   gaming                           float64\n",
      " 81   clubbing                         float64\n",
      " 82   reading                          float64\n",
      " 83   tv                               float64\n",
      " 84   theater                          float64\n",
      " 85   movies                           float64\n",
      " 86   concerts                         float64\n",
      " 87   music                            float64\n",
      " 88   shopping                         float64\n",
      " 89   yoga                             float64\n",
      " 90   d_sports                         object \n",
      " 91   d_tvsports                       object \n",
      " 92   d_exercise                       object \n",
      " 93   d_dining                         object \n",
      " 94   d_museums                        object \n",
      " 95   d_art                            object \n",
      " 96   d_hiking                         object \n",
      " 97   d_gaming                         object \n",
      " 98   d_clubbing                       object \n",
      " 99   d_reading                        object \n",
      " 100  d_tv                             object \n",
      " 101  d_theater                        object \n",
      " 102  d_movies                         object \n",
      " 103  d_concerts                       object \n",
      " 104  d_music                          object \n",
      " 105  d_shopping                       object \n",
      " 106  d_yoga                           object \n",
      " 107  interests_correlate              float64\n",
      " 108  d_interests_correlate            object \n",
      " 109  expected_happy_with_sd_people    float64\n",
      " 110  expected_num_interested_in_me    float64\n",
      " 111  expected_num_matches             float64\n",
      " 112  d_expected_happy_with_sd_people  object \n",
      " 113  d_expected_num_interested_in_me  object \n",
      " 114  d_expected_num_matches           object \n",
      " 115  like                             float64\n",
      " 116  guess_prob_liked                 float64\n",
      " 117  d_like                           object \n",
      " 118  d_guess_prob_liked               object \n",
      " 119  met                              float64\n",
      " 120  decision                         int8   \n",
      " 121  decision_o                       int8   \n",
      " 122  match                            int8   \n",
      "dtypes: float64(57), int8(7), object(59)\n",
      "memory usage: 7.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info(verbose=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Business                            0.062658\n",
       "MBA                                 0.056284\n",
       "Law                                 0.055562\n",
       "'Social Work'                       0.045460\n",
       "'International Affairs'             0.030307\n",
       "                                      ...   \n",
       "'Business [Finance & Marketing]'    0.000722\n",
       "Stats                               0.000722\n",
       "'MFA  Poetry'                       0.000722\n",
       "'marine geophysics'                 0.000601\n",
       "theory                              0.000601\n",
       "Name: field, Length: 259, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We're only concerened with 'race','gender' and 'race_o', since 'field' has too many unique values and we'll drop it anyways.\n",
    "df['field'].value_counts(normalize=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>race_african_american</th>\n",
       "      <th>race_latino</th>\n",
       "      <th>race_european</th>\n",
       "      <th>race_other</th>\n",
       "      <th>male</th>\n",
       "      <th>race_o_african_american</th>\n",
       "      <th>race_o_latino</th>\n",
       "      <th>race_o_european</th>\n",
       "      <th>race_o_other</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8373</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8374</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8375</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8376</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8377</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8378 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      race_african_american  race_latino  race_european  race_other  male  \\\n",
       "0                         0            0              0           0     0   \n",
       "1                         0            0              0           0     0   \n",
       "2                         0            0              0           0     0   \n",
       "3                         0            0              0           0     0   \n",
       "4                         0            0              0           0     0   \n",
       "...                     ...          ...            ...         ...   ...   \n",
       "8373                      0            0              1           0     1   \n",
       "8374                      0            0              1           0     1   \n",
       "8375                      0            0              1           0     1   \n",
       "8376                      0            0              1           0     1   \n",
       "8377                      0            0              1           0     1   \n",
       "\n",
       "      race_o_african_american  race_o_latino  race_o_european  race_o_other  \n",
       "0                           0              0                1             0  \n",
       "1                           0              0                1             0  \n",
       "2                           0              0                0             0  \n",
       "3                           0              0                1             0  \n",
       "4                           0              1                0             0  \n",
       "...                       ...            ...              ...           ...  \n",
       "8373                        0              1                0             0  \n",
       "8374                        0              0                0             1  \n",
       "8375                        0              1                0             0  \n",
       "8376                        0              0                0             0  \n",
       "8377                        0              0                0             0  \n",
       "\n",
       "[8378 rows x 9 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_cols= ['race_african_american','race_latino','race_european',\n",
    "    'race_other','male','race_o_african_american',\n",
    "    'race_o_latino','race_o_european','race_o_other']\n",
    "df[new_cols] = pd.get_dummies(\n",
    "    df[\n",
    "        ['race','gender','race_o']\n",
    "        ],\n",
    "        drop_first=True,\n",
    "    )\n",
    "df[new_cols] #Looks fine"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I can drop the original columns. \n",
    "I'll also be dropping columns containing \"d_\", since they were generated for the original research papers and will not be of use for our analysis.\n",
    "Finally, descriptive columns such as \"has_null\" will also be removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = ['race','gender','race_o','has_null','field']\n",
    "to_drop.extend(col for col in df.columns if col.startswith('d_'))\n",
    "df.drop(columns=to_drop,inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, some columns were created in a way that is not helpful to our investigation. Those which contain information about each\n",
    "participant's importance given to certain attributes. They were asked to rate 6 attributes (attractiveness, sincereness,\n",
    "sense of humor, intelligence, ambition, and sharing interests) on a *shared* scale from 0 to 100. In order to have more objective ratings, we came up with a ranking system that narrows down the scales to only 1-6. In this case, 1 represents the highest rated attribute and 6 the lowest rated one. Ties are broken using the average. More details can be found in the \"processing_tools\" module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>attractive_important</th>\n",
       "      <th>sincere_important</th>\n",
       "      <th>intellicence_important</th>\n",
       "      <th>funny_important</th>\n",
       "      <th>ambition_important</th>\n",
       "      <th>shared_interests_important</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8373</th>\n",
       "      <td>70.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8374</th>\n",
       "      <td>70.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8375</th>\n",
       "      <td>70.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8376</th>\n",
       "      <td>70.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8377</th>\n",
       "      <td>70.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8378 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      attractive_important  sincere_important  intellicence_important  \\\n",
       "0                     15.0               20.0                    20.0   \n",
       "1                     15.0               20.0                    20.0   \n",
       "2                     15.0               20.0                    20.0   \n",
       "3                     15.0               20.0                    20.0   \n",
       "4                     15.0               20.0                    20.0   \n",
       "...                    ...                ...                     ...   \n",
       "8373                  70.0                0.0                    15.0   \n",
       "8374                  70.0                0.0                    15.0   \n",
       "8375                  70.0                0.0                    15.0   \n",
       "8376                  70.0                0.0                    15.0   \n",
       "8377                  70.0                0.0                    15.0   \n",
       "\n",
       "      funny_important  ambition_important  shared_interests_important  \n",
       "0                15.0                15.0                        15.0  \n",
       "1                15.0                15.0                        15.0  \n",
       "2                15.0                15.0                        15.0  \n",
       "3                15.0                15.0                        15.0  \n",
       "4                15.0                15.0                        15.0  \n",
       "...               ...                 ...                         ...  \n",
       "8373             15.0                 0.0                         0.0  \n",
       "8374             15.0                 0.0                         0.0  \n",
       "8375             15.0                 0.0                         0.0  \n",
       "8376             15.0                 0.0                         0.0  \n",
       "8377             15.0                 0.0                         0.0  \n",
       "\n",
       "[8378 rows x 6 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's check them out:\n",
    "important_cols = [i for i in df.columns if 'important' in i]\n",
    "df[important_cols]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we transform them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = df.dropna(subset=important_cols).copy() #Doesn't work with nans. We'll handle those in future notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset[important_cols] = subset.apply(lambda x: rank_cols(\n",
    "    x[important_cols[0]],x[important_cols[1]],\n",
    "    x[important_cols[2]],x[important_cols[3]],\n",
    "    x[important_cols[4]],x[important_cols[5]],\n",
    "    keys=important_cols),axis=1).values.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#And now we just replace the original columns with the new ones\n",
    "df[important_cols] = subset[important_cols]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's see the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>attractive_important</th>\n",
       "      <th>sincere_important</th>\n",
       "      <th>intellicence_important</th>\n",
       "      <th>funny_important</th>\n",
       "      <th>ambition_important</th>\n",
       "      <th>shared_interests_important</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8373</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8374</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8375</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8376</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8377</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8378 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      attractive_important  sincere_important  intellicence_important  \\\n",
       "0                      4.5                1.5                     1.5   \n",
       "1                      4.5                1.5                     1.5   \n",
       "2                      4.5                1.5                     1.5   \n",
       "3                      4.5                1.5                     1.5   \n",
       "4                      4.5                1.5                     1.5   \n",
       "...                    ...                ...                     ...   \n",
       "8373                   1.0                5.0                     2.5   \n",
       "8374                   1.0                5.0                     2.5   \n",
       "8375                   1.0                5.0                     2.5   \n",
       "8376                   1.0                5.0                     2.5   \n",
       "8377                   1.0                5.0                     2.5   \n",
       "\n",
       "      funny_important  ambition_important  shared_interests_important  \n",
       "0                 4.5                 4.5                         4.5  \n",
       "1                 4.5                 4.5                         4.5  \n",
       "2                 4.5                 4.5                         4.5  \n",
       "3                 4.5                 4.5                         4.5  \n",
       "4                 4.5                 4.5                         4.5  \n",
       "...               ...                 ...                         ...  \n",
       "8373              2.5                 5.0                         5.0  \n",
       "8374              2.5                 5.0                         5.0  \n",
       "8375              2.5                 5.0                         5.0  \n",
       "8376              2.5                 5.0                         5.0  \n",
       "8377              2.5                 5.0                         5.0  \n",
       "\n",
       "[8378 rows x 6 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[important_cols]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I repeat the same proceedure for the columns that correspond to the subject's partner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_o = [i for i in df.columns if 'pref_o' in i]\n",
    "subset_2 = df.dropna(subset=important_o).copy() #Doesn't work with nans. We'll handle those in future notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_2[important_o] = subset_2.apply(lambda x: rank_cols(\n",
    "    x[important_o[0]],x[important_o[1]],\n",
    "    x[important_o[2]],x[important_o[3]],\n",
    "    x[important_o[4]],x[important_o[5]],\n",
    "    keys=important_o),axis=1).values.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[important_o] = subset_2[important_o]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pref_o_attractive</th>\n",
       "      <th>pref_o_sincere</th>\n",
       "      <th>pref_o_intelligence</th>\n",
       "      <th>pref_o_funny</th>\n",
       "      <th>pref_o_ambitious</th>\n",
       "      <th>pref_o_shared_interests</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8373</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8374</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>5.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>5.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8375</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8376</th>\n",
       "      <td>5.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8377</th>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8378 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      pref_o_attractive  pref_o_sincere  pref_o_intelligence  pref_o_funny  \\\n",
       "0                   1.0             3.0                  3.0           3.0   \n",
       "1                   1.0             4.5                  4.5           2.0   \n",
       "2                   1.5             3.5                  1.5           3.5   \n",
       "3                   2.0             5.0                  3.0           1.0   \n",
       "4                   1.0             5.0                  2.5           5.0   \n",
       "...                 ...             ...                  ...           ...   \n",
       "8373                5.0             5.0                  1.0           2.0   \n",
       "8374                1.0             2.0                  3.5           5.5   \n",
       "8375                NaN             NaN                  NaN           NaN   \n",
       "8376                5.0             1.5                  1.5           5.0   \n",
       "8377                2.5             2.5                  5.0           4.0   \n",
       "\n",
       "      pref_o_ambitious  pref_o_shared_interests  \n",
       "0                  6.0                      5.0  \n",
       "1                  4.5                      4.5  \n",
       "2                  5.0                      6.0  \n",
       "3                  5.0                      5.0  \n",
       "4                  5.0                      2.5  \n",
       "...                ...                      ...  \n",
       "8373               5.0                      3.0  \n",
       "8374               3.5                      5.5  \n",
       "8375               NaN                      NaN  \n",
       "8376               5.0                      3.0  \n",
       "8377               6.0                      1.0  \n",
       "\n",
       "[8378 rows x 6 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[important_o]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final steps before splitting into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = ['like','match','wave']\n",
    "to_drop.extend([i for i in df.columns if i.startswith('d_')])\n",
    "df.drop(columns=to_drop,inplace=True) #Some columns are not useful for our experiment. Columns containing \"d_\", for example, don't add information to the models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to identify columns which can be rounded *after* imputing nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_round = spot_round_cols(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df.drop(columns=['decision']).values,df['decision'].values, \n",
    "                                                    stratify=df[['decision','male']].values, \n",
    "                                                    test_size=0.1)\n",
    "X_train = pd.DataFrame(X_train,columns=df.drop(columns=['decision']).columns)\n",
    "y_train = pd.DataFrame(y_train, columns=['decision'])\n",
    "X_test = pd.DataFrame(X_test,columns=df.drop(columns=['decision']).columns)\n",
    "y_test = pd.DataFrame(y_test, columns=['decision'])\n",
    "test = pd.concat([y_test,X_test],axis=1) #We set aside a validation set from the beginning, again, to avoid any kind of data leakage.\n",
    "df = pd.concat([X_train,y_train],axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, null imputation. Very important to do it after splitting to avoid data leakage"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data imputation involves a certain degree of randomness. Because of this, results may differ a little from those pusblished in the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[IterativeImputer] Completing matrix with shape (7540, 67)\n",
      "[IterativeImputer] Ending imputation round 1/10, elapsed time 183.18\n",
      "[IterativeImputer] Change: 26.496752714007066, scaled tolerance: 0.055 \n",
      "[IterativeImputer] Ending imputation round 2/10, elapsed time 359.43\n",
      "[IterativeImputer] Change: 24.534499999999998, scaled tolerance: 0.055 \n",
      "[IterativeImputer] Ending imputation round 3/10, elapsed time 547.52\n",
      "[IterativeImputer] Change: 22.498400000000004, scaled tolerance: 0.055 \n",
      "[IterativeImputer] Ending imputation round 4/10, elapsed time 748.07\n",
      "[IterativeImputer] Change: 21.1083, scaled tolerance: 0.055 \n",
      "[IterativeImputer] Ending imputation round 5/10, elapsed time 943.54\n",
      "[IterativeImputer] Change: 20.4061, scaled tolerance: 0.055 \n",
      "[IterativeImputer] Ending imputation round 6/10, elapsed time 1157.57\n",
      "[IterativeImputer] Change: 23.4381, scaled tolerance: 0.055 \n",
      "[IterativeImputer] Ending imputation round 7/10, elapsed time 1389.96\n",
      "[IterativeImputer] Change: 22.651499999999995, scaled tolerance: 0.055 \n",
      "[IterativeImputer] Ending imputation round 8/10, elapsed time 1677.44\n",
      "[IterativeImputer] Change: 31.179199999999994, scaled tolerance: 0.055 \n",
      "[IterativeImputer] Ending imputation round 9/10, elapsed time 2519.22\n",
      "[IterativeImputer] Change: 28.691799999999997, scaled tolerance: 0.055 \n",
      "[IterativeImputer] Ending imputation round 10/10, elapsed time 3114.03\n",
      "[IterativeImputer] Change: 21.4309, scaled tolerance: 0.055 \n",
      "[IterativeImputer] Completing matrix with shape (7540, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Laureano\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\impute\\_iterative.py:785: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Laureano\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:402: UserWarning: X has feature names, but IterativeImputer was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[IterativeImputer] Ending imputation round 1/10, elapsed time 2.81\n",
      "[IterativeImputer] Ending imputation round 2/10, elapsed time 5.56\n",
      "[IterativeImputer] Ending imputation round 3/10, elapsed time 8.16\n",
      "[IterativeImputer] Ending imputation round 4/10, elapsed time 11.01\n",
      "[IterativeImputer] Ending imputation round 5/10, elapsed time 13.66\n",
      "[IterativeImputer] Ending imputation round 6/10, elapsed time 16.57\n",
      "[IterativeImputer] Ending imputation round 7/10, elapsed time 19.04\n",
      "[IterativeImputer] Ending imputation round 8/10, elapsed time 21.34\n",
      "[IterativeImputer] Ending imputation round 9/10, elapsed time 23.78\n",
      "[IterativeImputer] Ending imputation round 10/10, elapsed time 26.05\n",
      "[IterativeImputer] Completing matrix with shape (838, 67)\n",
      "[IterativeImputer] Ending imputation round 1/10, elapsed time 24.95\n",
      "[IterativeImputer] Change: 16.948134260685354, scaled tolerance: 0.055 \n",
      "[IterativeImputer] Ending imputation round 2/10, elapsed time 50.81\n",
      "[IterativeImputer] Change: 12.439700000000002, scaled tolerance: 0.055 \n",
      "[IterativeImputer] Ending imputation round 3/10, elapsed time 77.11\n",
      "[IterativeImputer] Change: 12.566299999999998, scaled tolerance: 0.055 \n",
      "[IterativeImputer] Ending imputation round 4/10, elapsed time 103.21\n",
      "[IterativeImputer] Change: 10.622, scaled tolerance: 0.055 \n",
      "[IterativeImputer] Ending imputation round 5/10, elapsed time 129.08\n",
      "[IterativeImputer] Change: 7.6261000000000045, scaled tolerance: 0.055 \n",
      "[IterativeImputer] Ending imputation round 6/10, elapsed time 155.59\n",
      "[IterativeImputer] Change: 9.6726, scaled tolerance: 0.055 \n",
      "[IterativeImputer] Ending imputation round 7/10, elapsed time 181.50\n",
      "[IterativeImputer] Change: 10.019900000000002, scaled tolerance: 0.055 \n",
      "[IterativeImputer] Ending imputation round 8/10, elapsed time 207.54\n",
      "[IterativeImputer] Change: 10.614900000000002, scaled tolerance: 0.055 \n",
      "[IterativeImputer] Ending imputation round 9/10, elapsed time 233.75\n",
      "[IterativeImputer] Change: 9.032100000000002, scaled tolerance: 0.055 \n",
      "[IterativeImputer] Ending imputation round 10/10, elapsed time 259.55\n",
      "[IterativeImputer] Change: 10.3171, scaled tolerance: 0.055 \n",
      "[IterativeImputer] Completing matrix with shape (838, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Laureano\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\impute\\_iterative.py:785: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Laureano\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:402: UserWarning: X has feature names, but IterativeImputer was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[IterativeImputer] Ending imputation round 1/10, elapsed time 0.77\n",
      "[IterativeImputer] Ending imputation round 2/10, elapsed time 1.56\n",
      "[IterativeImputer] Ending imputation round 3/10, elapsed time 2.35\n",
      "[IterativeImputer] Ending imputation round 4/10, elapsed time 3.14\n",
      "[IterativeImputer] Ending imputation round 5/10, elapsed time 3.99\n",
      "[IterativeImputer] Ending imputation round 6/10, elapsed time 4.79\n",
      "[IterativeImputer] Ending imputation round 7/10, elapsed time 5.56\n",
      "[IterativeImputer] Ending imputation round 8/10, elapsed time 6.35\n",
      "[IterativeImputer] Ending imputation round 9/10, elapsed time 7.12\n",
      "[IterativeImputer] Ending imputation round 10/10, elapsed time 7.88\n"
     ]
    }
   ],
   "source": [
    "df = impute_nulls(df)\n",
    "test = impute_nulls(test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rounding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in cols_to_round: \n",
    "    try:\n",
    "        test.loc[:,(col)] = np.round(test[col]).astype('int8') \n",
    "        df.loc[:,(col)] = np.round(df[col]).astype('int8')\n",
    "    except:\n",
    "        print(col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['met'] = np.where(df['met']>1,1,df['met']) #Setting a limit manually for some columns (Hard to set specific columns on the sklearn imputer)\n",
    "test['met'] = np.where(test['met']>1,1,test['met'])\n",
    "for i in ['funny_o','gaming','reading']:\n",
    "    df[i] = np.where(df[i]>10,10,df[i])\n",
    "    test[i] = np.where(test[i]>10,10,test[i])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A duplicate of the column \"decision\" was created in the train-test split. We'll simply remove it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>decision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>833</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>834</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>835</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>837</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>838 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     decision\n",
       "0           0\n",
       "1           0\n",
       "2           0\n",
       "3           0\n",
       "4           0\n",
       "..        ...\n",
       "833         0\n",
       "834         1\n",
       "835         1\n",
       "836         1\n",
       "837         0\n",
       "\n",
       "[838 rows x 1 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "test.loc[:,test.columns.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.loc[:,~test.columns.duplicated()]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datasets are now clean enough for further analysis. I'll save the results in parquet format to keep the object types \n",
    "(plus it's a bit more memory-efficient)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '../Datasets//df_imput.parquet'\n",
    "test_path = '../Datasets//test_imput.parquet'\n",
    "df.to_parquet(train_path)\n",
    "test.to_parquet(test_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2c90431c23e0166251fd9d62ee3a0b9d582bd4ca2aa65bf16ce65b80610b1f97"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
